{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15331c7",
   "metadata": {
    "id": "e15331c7"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676f1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "    text_returned = re.sub(regex,' ',text)\n",
    "    return text_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f583de89",
   "metadata": {
    "id": "f583de89"
   },
   "outputs": [],
   "source": [
    "files = glob.glob('../Data files required/english-corpora/*.txt', recursive = True)\n",
    "ps = PorterStemmer()\n",
    "tk = WhitespaceTokenizer()\n",
    "data = {}\n",
    "i = 0\n",
    "for file in files:\n",
    "#     print(file,'\\n')\n",
    "    file1 = open(file,\"r\")\n",
    "    text1 = file1.read()\n",
    "    cleaned = remove_special_characters(text1)\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    cleaned = cleaned.replace(\"\\t\",\" \")\n",
    "    cleaned = re.sub('\\s+',' ',cleaned)\n",
    "    cleaned = cleaned.lower()\n",
    "    token = tk.tokenize(cleaned)\n",
    "    stemming_output = ' '.join([ps.stem(w) for w in token])\n",
    "    name = file[16:-4]\n",
    "    data[name] = stemming_output\n",
    "#     print(data[name])\n",
    "    i += 1\n",
    "    if (i%300 == 0):\n",
    "        print('Document no equals ',i)\n",
    "#         break\n",
    "    break # Kept intentionally\n",
    "# print(data['C00010'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00436d7b",
   "metadata": {},
   "source": [
    "'''Since the above code was taking time for executing so I just ran it once and saved it as a pickle. So in order to stop it from running again I kept break in for loop intentionally and loaded the previous saved output in the variable required. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4NQ1dWxlquSs",
   "metadata": {
    "id": "4NQ1dWxlquSs"
   },
   "outputs": [],
   "source": [
    "with open('../Data files required/Processed_corpos_IR.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b57ca77",
   "metadata": {},
   "source": [
    "#### Making an index dictionary to number every document: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "TrR27Del29H5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TrR27Del29H5",
    "outputId": "3931eafa-3450-4859-9c8c-95f8f79cc173"
   },
   "outputs": [],
   "source": [
    "index_dict = {}\n",
    "i = 0\n",
    "for file in data.keys():\n",
    "    key = file\n",
    "    index_dict[key] = i\n",
    "    i += 1\n",
    "    # if i == 5000:\n",
    "    #     break\n",
    "# print(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38FBFGoY-_uL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38FBFGoY-_uL",
    "outputId": "63ab65c7-bcd3-44fc-87aa-ad50b87e7021"
   },
   "outputs": [],
   "source": [
    "index_dict2 = {v: k for k, v in index_dict.items()}\n",
    "# print(index_dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hJDEb0FH1xxU",
   "metadata": {
    "id": "hJDEb0FH1xxU"
   },
   "source": [
    "#### Making a dict having info about no of words in each document.\n",
    "#### Also calculating average no of words accross all documents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "TbMZPXG-1xES",
   "metadata": {
    "id": "TbMZPXG-1xES"
   },
   "outputs": [],
   "source": [
    "No_W_doc_dict = {}\n",
    "sum_ = 0\n",
    "for doc in data.keys():\n",
    "    No_W_doc_dict[doc] = len(data[doc])\n",
    "    sum_ += len(data[doc])\n",
    "Average_no_words = sum_ / 8351\n",
    "    # print(doc)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b64d97",
   "metadata": {
    "id": "49b64d97"
   },
   "source": [
    "## Making posting list : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18fd8f86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18fd8f86",
    "outputId": "4aee2071-4e70-4d02-a32c-6229953d71da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_words = []\n",
    "unique_words_dict = {}\n",
    "i = 0\n",
    "for key in data.keys():\n",
    "    i += 1\n",
    "    for word in data[key].split():\n",
    "        if unique_words_dict.get(word, 0)  == 0:\n",
    "            unique_words.append(word)\n",
    "            unique_words_dict[word] = 1\n",
    "        else:\n",
    "            pass\n",
    "#     if i%300 == 0:\n",
    "#         print('Document no ',i,' and unique words till now equals ',len(unique_words))\n",
    "# len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "_AoKM4w8v1X9",
   "metadata": {
    "id": "_AoKM4w8v1X9"
   },
   "outputs": [],
   "source": [
    "# unique_words = []\n",
    "# unique_words_dict = {}\n",
    "temp_dict = {el:{} for el in unique_words}\n",
    "# dict2 = {}\n",
    "i = 0\n",
    "for key in data.keys():\n",
    "    i += 1\n",
    "    for word in data[key].split():\n",
    "        count = data[key].count(word)\n",
    "        temp_dict[word][key] = count\n",
    "        break # Kept intentionally\n",
    "    if i%300 == 0:\n",
    "        print('Document completed till now equals ',i)\n",
    "    break # Kept intentionally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cO_8bvdzoj9P",
   "metadata": {
    "id": "cO_8bvdzoj9P"
   },
   "outputs": [],
   "source": [
    "with open('../Data files required/Saved_dictionary_IR.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ty-Q5M-mti8u",
   "metadata": {
    "id": "ty-Q5M-mti8u"
   },
   "source": [
    "### Making dict to fetch index no for every word and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hgkIFQFctij1",
   "metadata": {
    "id": "hgkIFQFctij1"
   },
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "i = 0\n",
    "for word in unique_words:\n",
    "    word_index[word] = i\n",
    "    i += 1\n",
    "\n",
    "# word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1yGh1RYVtie2",
   "metadata": {
    "id": "1yGh1RYVtie2"
   },
   "outputs": [],
   "source": [
    "index_word = {v: k for k, v in word_index.items()}\n",
    "# len(index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Md0engin1X7k",
   "metadata": {
    "id": "Md0engin1X7k"
   },
   "source": [
    "### Calculating IDF for every word and storing it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "SxFR8Y501Xju",
   "metadata": {
    "id": "SxFR8Y501Xju"
   },
   "outputs": [],
   "source": [
    "word_IDF_dict = {}\n",
    "for word in unique_words:\n",
    "    word_IDF_dict[word] = np.log2((8351 + 1) / (len(loaded_dict[word].keys()) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mvN5gb63xb0i",
   "metadata": {
    "id": "mvN5gb63xb0i"
   },
   "source": [
    "### Making document norm required for TF-IDF model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "kbuyd5lQxjms",
   "metadata": {
    "id": "kbuyd5lQxjms"
   },
   "outputs": [],
   "source": [
    "document_norm_dict = {}\n",
    "for doc in data.keys():\n",
    "    vector = np.zeros(len(unique_words), dtype = int)\n",
    "    for word in unique_words:\n",
    "        indx = word_index[word]\n",
    "        try:\n",
    "            tf = loaded_dict[word][doc]\n",
    "        except:\n",
    "            tf = 0\n",
    "        vector[indx] = tf * word_IDF_dict[word]\n",
    "        break   #  Kept intentionally\n",
    "    norm = np.linalg.norm(vector)\n",
    "    document_norm_dict[doc] = norm\n",
    "    break       #  Kept intentionally\n",
    "\n",
    "# document_norm_dict['S00602'][100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "yOcXctsFkj4x",
   "metadata": {
    "id": "yOcXctsFkj4x"
   },
   "outputs": [],
   "source": [
    "with open('../Data files required/document_norm_dict.pkl', 'rb') as f:\n",
    "    document_norm_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZoCoPrWExAYM",
   "metadata": {
    "id": "ZoCoPrWExAYM"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff53f1",
   "metadata": {},
   "source": [
    "### Taking input file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "559bc9e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if len(sys.argv)<=1:\n",
    "#     print('Error. Please try writing in this format: python3 code.py test.csv')\n",
    "#     exit()\n",
    "\n",
    "# input_file = pd.read_csv(sys.argv[1], sep=\"\\t\", header = None)\n",
    "\n",
    "input_file = pd.read_csv(\"Input.txt\",sep=\"\\t\", header=None)\n",
    "\n",
    "# input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "GWeh4T8ZgVAa",
   "metadata": {
    "id": "GWeh4T8ZgVAa"
   },
   "outputs": [],
   "source": [
    "def clean(query):\n",
    "    regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "    cleaned = re.sub(regex,' ',query)\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    cleaned = cleaned.replace(\"\\t\",\" \")\n",
    "    cleaned = re.sub('\\s+',' ',cleaned)\n",
    "    cleaned = cleaned.lower()\n",
    "    token = tk.tokenize(cleaned)\n",
    "    processed_query = ' '.join([ps.stem(w) for w in token])\n",
    "    return processed_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25091ddd",
   "metadata": {},
   "source": [
    "### (a) Boolean retrival model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c87vPaNNYlYV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c87vPaNNYlYV",
    "outputId": "5e7019b0-b865-44ac-ee00-b79d22cb8a99"
   },
   "outputs": [],
   "source": [
    "# print(query)\n",
    "column = ['QueryId', 'Iteration', 'DocId', 'Relevance']\n",
    "BRS_output = pd.DataFrame(columns=column, index=range(99)) \n",
    "a = 0\n",
    "for r in range(20):\n",
    "    query = input_file.iat[r,1]\n",
    "    query = clean(query)\n",
    "    connecting_words = []\n",
    "    cnt = 1\n",
    "    different_words = []\n",
    "    for word in query.split():\n",
    "        if word.lower() != \"and\" and word.lower() != \"or\" and word.lower() != \"not\":\n",
    "            different_words.append(word.lower())\n",
    "        else:\n",
    "            connecting_words.append(word.lower())\n",
    "    # print(different_words)\n",
    "    # print(connecting_words)\n",
    "\n",
    "    bool_dict = {}\n",
    "    # arr2 = np.zeros(3, dtype=bool)\n",
    "    for word in different_words:\n",
    "        # print(word)\n",
    "        if word in unique_words:\n",
    "            i = 0\n",
    "            bool_dict[word] = np.zeros(8351, dtype=bool)\n",
    "            documents = list(loaded_dict[word].keys())\n",
    "            indices = np.zeros(len(documents))\n",
    "            for x in documents:\n",
    "                try:\n",
    "                    indices[i] = index_dict[x]\n",
    "                    i += 1\n",
    "                except:\n",
    "                    x = x[:-4]\n",
    "                    indices[i] = index_dict[x]\n",
    "                    i += 1\n",
    "            for ind in indices:\n",
    "                ind = int(ind)\n",
    "                bool_dict[word][ind] = 1\n",
    "\n",
    "    # print(len(documents), len(indices))\n",
    "    # print(bool_dict)\n",
    "    # bool_dict['cold'][40]\n",
    "\n",
    "    query_list = query.split()\n",
    "    if len(query_list) == 1:\n",
    "        value = query_list[0]\n",
    "    # query_list\n",
    "    \n",
    "    ########## Processing NOT in query ###########\n",
    "\n",
    "    i = 0\n",
    "    for word in query_list:\n",
    "        if word == 'not':\n",
    "            next_word = query_list[i+1]\n",
    "            query_list.remove(word)\n",
    "            bool_dict[next_word] = ~ bool_dict[next_word]\n",
    "        i += 1\n",
    "    # print(query_list)\n",
    "    # bool_dict\n",
    "    \n",
    "    ########## Processing AND in query ###########\n",
    "\n",
    "    i = 0\n",
    "    j = 1\n",
    "    while 'and' in  query_list:\n",
    "        word = query_list[i]\n",
    "    #     print(word)\n",
    "        if word == 'and':\n",
    "            prev_word = query_list[i-1]\n",
    "            # print(\"i equals : \",i)\n",
    "            next_word = query_list[i+1]\n",
    "            # print('Prev and next equals : ',prev_word,' ',next_word)\n",
    "            value = 'ans' + str(j)\n",
    "            j += 1\n",
    "            query_list[i] = value\n",
    "            # print(query_list)\n",
    "            query_list.remove(prev_word)\n",
    "            # print(\"i equals : \",i)\n",
    "            # print(query_list)\n",
    "            query_list.remove(next_word)\n",
    "            bool_dict[value] =  bool_dict[next_word] & bool_dict[prev_word]\n",
    "            i = 0\n",
    "        else :\n",
    "            i += 1\n",
    "\n",
    "    # print(\"Query list equals : \",query_list)\n",
    "    # print(bool_dict)\n",
    "    # print(j)\n",
    "    \n",
    "    ########## Processing OR in query ###########\n",
    "\n",
    "    i = 0\n",
    "    while 'or' in  query_list:\n",
    "        word = query_list[i]\n",
    "    #     print(word)\n",
    "        if word == 'or':\n",
    "            prev_word = query_list[i-1]\n",
    "            # print(\"i equals : \",i)\n",
    "            next_word = query_list[i+1]\n",
    "            # print('Prev and next equals : ',prev_word,' ',next_word)\n",
    "            value = 'ans' + str(j)\n",
    "            j += 1\n",
    "            query_list[i] = value\n",
    "            # print(query_list)\n",
    "            query_list.remove(prev_word)\n",
    "            # print(\"i equals : \",i)\n",
    "            # print(query_list)\n",
    "            query_list.remove(next_word)\n",
    "            bool_dict[value] =  bool_dict[next_word] | bool_dict[prev_word]\n",
    "            i = 0\n",
    "        else :\n",
    "            i += 1\n",
    "        # if i >= len(query_list):\n",
    "        #     i = 0\n",
    "\n",
    "    # print(\"Query list equals : \",query_list)\n",
    "    # print(bool_dict)\n",
    "    # print(j)\n",
    "    \n",
    "    ########## Processing words without any operator between them in query ###########\n",
    "    \n",
    "    i = 0\n",
    "    while len(query_list) != 1:\n",
    "        word = query_list[i]\n",
    "        # print(word)\n",
    "        prev_word = query_list[i]\n",
    "        # print(\"i equals : \",i)\n",
    "        next_word = query_list[i+1]\n",
    "        # print('Prev and next equals : ',prev_word,' ',next_word)\n",
    "        value = 'ans' + str(j)\n",
    "        j += 1\n",
    "        query_list[0] = value\n",
    "        # query_list.remove(prev_word)\n",
    "        # print(\"i equals : \",i)\n",
    "        # print(query_list)\n",
    "        query_list.remove(next_word)\n",
    "        # print(query_list)\n",
    "\n",
    "        bool_dict[value] =  bool_dict[next_word] & bool_dict[prev_word]\n",
    "        # i = 0\n",
    "\n",
    "    # print(\"Query list equals : \",query_list)\n",
    "    # print(bool_dict)\n",
    "    # print(j)\n",
    "\n",
    "    ans = bool_dict[value].copy()\n",
    "    indx = np.where(ans)[0]\n",
    "    # final_ans = \"Appropriate documents for given query are :  \"\n",
    "    # for x in indx:\n",
    "    #     final_ans += index_dict2[x] + ', '\n",
    "    # final_ans\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for x in indx:\n",
    "        Qid = \"Q\" + str(r+1)\n",
    "        BRS_output.loc[a].QueryId = Qid\n",
    "        BRS_output.loc[a].Iteration = 1\n",
    "        BRS_output.loc[a].DocId = index_dict2[x]\n",
    "        BRS_output.loc[a].Relevance = 1        \n",
    "        count += 1\n",
    "        a += 1\n",
    "        if count == 5:\n",
    "            break\n",
    "\n",
    "BRS_output = BRS_output.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f947926",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRS_output.to_csv('BRS_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v4SBeZLEl4Vz",
   "metadata": {
    "id": "v4SBeZLEl4Vz"
   },
   "source": [
    "### (b) (20 marks) Implement a system from the Tf-Idf family with appropriate forms for the functions and tuned parameters. A query is matched using cosine similarity. : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "BSRgMhdCLCmg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSRgMhdCLCmg",
    "outputId": "a3cfb87b-edcd-4d0b-910c-a731eb42c41f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22428/4266537202.py:44: RuntimeWarning: invalid value encountered in true_divide\n",
      "  doc_vector = np.array(doc_vector) / document_norm_dict[doc]\n"
     ]
    }
   ],
   "source": [
    "# query_vector = []\n",
    "\n",
    "# column = ['QueryId', 'Iteration', 'DocId', 'Relevance']\n",
    "TfIdf_output = pd.DataFrame(columns=column, index=range(100)) \n",
    "a = 0\n",
    "for r in range(20):\n",
    "    query = input_file.iat[r,1]\n",
    "    query = clean(query)\n",
    "    query_list = query.split()\n",
    "    query_vector = []\n",
    "    for word in query_list:\n",
    "\n",
    "        if word in unique_words:\n",
    "            idf_word = word_IDF_dict[word]\n",
    "        else:\n",
    "            idf_word = 0\n",
    "\n",
    "        tf_idf = query_list.count(word) * idf_word\n",
    "        query_vector.append(tf_idf)\n",
    "    #     q_norm+=tf_idf**2\n",
    "    # q_norm=math.sqrt(q_norm)\n",
    "\n",
    "    query_norm = np.linalg.norm(query_vector)\n",
    "    query_vector = np.array(query_vector) / query_norm\n",
    "    # query_vector\n",
    "\n",
    "    # score = np.zeros(len(data.keys()))\n",
    "    score = {}\n",
    "    for i in index_dict2.keys():\n",
    "        doc_vector = []\n",
    "        for word in query_list:\n",
    "            doc = index_dict2[i]\n",
    "            if word in unique_words:\n",
    "                idf_word = word_IDF_dict[word]\n",
    "                try:\n",
    "                    tf = loaded_dict[word][doc]\n",
    "                except:\n",
    "                    tf = 0\n",
    "                tf_idf = tf * idf_word\n",
    "            else:\n",
    "                tf_idf = 0\n",
    "\n",
    "            doc_vector.append(tf_idf)\n",
    "        doc_vector = np.array(doc_vector) / document_norm_dict[doc]\n",
    "        key = np.dot(query_vector,doc_vector)\n",
    "        score[key] = doc\n",
    "\n",
    "    # score = sorted(score.items(),key=lambda x:x[1],reverse=True)\n",
    "\n",
    "    score_dict = collections.OrderedDict(sorted(score.items(),reverse=True))\n",
    "    # score_dict\n",
    "\n",
    "    # score_dict = collections.OrderedDict(sorted(score.items(),reverse=True))\n",
    "    # od.keys()\n",
    "    count = 0\n",
    "    for k,v in score_dict.items():\n",
    "        Qid = \"Q\" + str(r+1)\n",
    "        TfIdf_output.loc[a].QueryId = Qid\n",
    "        TfIdf_output.loc[a].Iteration = 1\n",
    "        TfIdf_output.loc[a].DocId = v\n",
    "        TfIdf_output.loc[a].Relevance = 1        \n",
    "        count += 1\n",
    "        a += 1\n",
    "        if count == 5:\n",
    "            break\n",
    "TfIdf_output = TfIdf_output.dropna()\n",
    "# TfIdf_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c68dd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "TfIdf_output.to_csv('TfIdf_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ml3k9t6wyrAa",
   "metadata": {
    "id": "Ml3k9t6wyrAa"
   },
   "source": [
    "### (c) (20 marks) Implement a system from the BM25 family with appropriate forms for the functions and tuned parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7JV8ouD9t36p",
   "metadata": {
    "id": "7JV8ouD9t36p"
   },
   "source": [
    "#### Calculating IDF (for every word ) required for BM25 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "jcO6_s2wy1kM",
   "metadata": {
    "id": "jcO6_s2wy1kM"
   },
   "outputs": [],
   "source": [
    "word_IDF_dict_BM25 = {}\n",
    "for word in unique_words:\n",
    "    freq = len(loaded_dict[word].keys())\n",
    "    num = 8531 - freq + 0.5\n",
    "    den = freq + 0.5\n",
    "    word_IDF_dict_BM25[word] = np.log(1 + num/den)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bgDY0D6VHd",
   "metadata": {
    "id": "15bgDY0D6VHd"
   },
   "source": [
    "##### BM25 started : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "TU_V0kCty1Z9",
   "metadata": {
    "id": "TU_V0kCty1Z9"
   },
   "outputs": [],
   "source": [
    "BM25_output = pd.DataFrame(columns=column, index=range(100)) \n",
    "a = 0\n",
    "for r in range(20):\n",
    "    query = input_file.iat[r,1]\n",
    "    query = clean(query)\n",
    "    query_list = query.split()\n",
    "    score_BM25 = {}\n",
    "    k = 1.2\n",
    "    b = 0.75\n",
    "    for doc in data.keys():\n",
    "        Sum = 0\n",
    "        for word in query_list:\n",
    "            try:\n",
    "                tf = loaded_dict[word][doc]\n",
    "                idf = word_IDF_dict_BM25[word]\n",
    "                num = idf * tf * (k + 1)\n",
    "                den = tf + k * (1 - b + b*No_W_doc_dict[doc]/Average_no_words )\n",
    "                Sum += num/den\n",
    "            except:\n",
    "                pass\n",
    "        score_BM25[Sum] = doc\n",
    "\n",
    "    score_dict_BM25 = collections.OrderedDict(sorted(score_BM25.items(),reverse=True))\n",
    "\n",
    "    count = 0\n",
    "    for k,v in score_dict_BM25.items():\n",
    "        Qid = \"Q\" + str(r+1)\n",
    "        BM25_output.loc[a].QueryId = Qid\n",
    "        BM25_output.loc[a].Iteration = 1\n",
    "        BM25_output.loc[a].DocId = v\n",
    "        BM25_output.loc[a].Relevance = 1        \n",
    "        count += 1\n",
    "        a += 1\n",
    "        if count == 5:\n",
    "            break\n",
    "            \n",
    "BM25_output = BM25_output.dropna()\n",
    "\n",
    "BM25_output.to_csv('BM25_output.csv', index=False)\n",
    "\n",
    "# with pd.option_context('display.max_rows', None,'display.max_columns', None):\n",
    "#     display(BM25_output)\n",
    "# BM25_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e517c756",
   "metadata": {},
   "source": [
    "### Finished"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IR Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
